#!/usr/bin/env python
# coding:utf-8

#
# 將聚會手記作後製整理
#
# Author: Chun-Yu Lee (Mat) <matlinuxer2@gmail.com>
# Copyright: Chun-Yu Lee (Mat) <matlinuxer2@gmail.com>
# License: MIT
#

from xmlrpclib import ServerProxy
from common import *
import re
import os
import subprocess
import tempfile
import user_data

ROOT = os.path.dirname(os.path.abspath(__file__))
cache_dir = os.path.join(ROOT, "cache")

#
# 程式參數
#
read_settings_from_file()
api_user = volatile_settings['wikidot_api_user']
api_key  = volatile_settings['wikidot_api_key']
site     = 'hackingthursday'
root_url = 'http://www.hackingthursday.org'
proxy    = ServerProxy('https://' + api_user + ':' + api_key + '@www.wikidot.com/xml-rpc-api.php')

#
# 程式資料結構
#
collection    = []
collection2   = []
collection3   = []
user_mapping  = user_data.data

#
# 將頁面資料從 wikidot 下載到本地端
#
def pull_and_cache_pages(site, pages):
    while pages.__len__() > 0:
        # 一次取10個
        page_ary = pages[0:10]
        meta_ary = proxy.pages.get_meta({"site": site, "pages": page_ary})

        for page in page_ary:
            page_time = meta_ary[page]['updated_at']
            page_file = os.path.join(cache_dir, page + '@' + page_time)

            if os.access(cache_dir, os.R_OK | os.W_OK) is False:
                os.mkdir(cache_dir)

            if os.access(page_file, os.R_OK) is False:
                exist_pages = os.listdir(cache_dir)
                for p in exist_pages:
                    if p[:page.__len__()] == page:
                        p_path = os.path.join(cache_dir, p)
                        print "Remove duplicate: ", p
                        os.remove(p_path)

                print "Fetching page: ", page, " ..."
                the_page = proxy.pages.get_one({"site": site, "page": page})
                content = the_page["content"].encode('utf8')
                f = open(page_file, 'w')
                f.write(content)
                f.close()

        # 跳下一輪
        pages = pages[10:]


def get_page_content(page):
    result = ""

    exist_pages = os.listdir(cache_dir)
    for p in exist_pages:
        if p[:page.__len__()+1] == page+"@":
            p_path = os.path.join(cache_dir, p)
            result = file2string(p_path)

    return result

def list_headings(page):
    result = [];

    exist_pages = os.listdir(cache_dir)
    for p in exist_pages:
        if p[:page.__len__()] == page:
            p_path = os.path.join(cache_dir, p)
            cmd = "php parse_wikidot_heading.php --file='%s'" % p_path
            out = subprocess.Popen( cmd, shell=True, stdout=subprocess.PIPE ).communicate()[0]
            result = out.strip().split('\n')

    return result


def append_data_per_author(author, date, data):
    global collection

    collection += [(author, date, data)]

def collect_unmapped_headding( heading, page ):
    global collection2

    collection2 += [(page, heading)]

def collect_notify( url_name ):
    global collection3

    collection3 += [ url_name ]


def normalize_alias( alias ):
    result = alias
    result = result.replace(':'  , '')
    result = result.replace(','  , '')
    result = result.replace('.'  , '')
    result = result.replace('-'  , '')
    result = result.replace('\'' , '')
    result = result.replace('"'  , '')
    result = result.replace('_'  , '')
    result = result.replace(' '  , '')
    result = result.lower() 
    result = result.strip()

    return result

def find_author_by_heading( heading ):
    global user_mapping

    result = None 

    alias = normalize_alias( heading )

    for row in user_mapping:
        aliases = row['alias']
        for a in aliases:
                a2 = normalize_alias( a )
                if alias == a2:
                        result = row

    return result

def find_author_by_name( url_name ):
    global user_mapping

    result = None 

    for row in user_mapping:
        if url_name == row['url_name']:
                result = row

    return result

def partition_page(page):
    content   = get_page_content(page)
    headings  = list_headings(page)
    
    pos_ary      = []
    parts        = []
    content_rows = content.split("\n")

    for i in range(0, content_rows.__len__()):
        line = content_rows[i]
        #print i,": ", line
        result = re.findall('^\+ (.*)\s*$', line)
        if result.__len__() > 0:
            heading = result[0]
            if heading in headings:
                pos_ary += [(heading, i)]

    for i in range(0, pos_ary.__len__()):
        data = []
        if i + 1 == pos_ary.__len__():  # 最後一個
            for j in range(pos_ary[i][1] + 1, content_rows.__len__()):
                data += [content_rows[j]]
        else:
            for j in range(pos_ary[i][1] + 1, pos_ary[i + 1][1]):
                data += [content_rows[j]]

        heading = pos_ary[i][0]
        parts += [( heading, data )]

    return parts

def parse_data_from_page(page):
    parts = partition_page( page )

    for part in parts:
        heading_str = part[0]
        data        = part[1]

        heading_ary = heading_str.split(',')
        for heading in heading_ary:
            heading = heading.strip()

            # 如果該段是作者，則加入資料收集        
            author_data = find_author_by_heading( heading )
            if author_data != None:
                author = author_data['url_name']
                print "mapped  :: %s/%s :: %s => %s " % ( root_url, page, heading, author )
                append_data_per_author(author, page, data)
            else:
                print "unmapped:: %s/%s :: %s " % ( root_url, page, heading )
                collect_unmapped_headding( heading, page ) 


def list_authors():
    result = []
    for item in user_mapping:
        name = item['url_name']
        if name in result:
            continue
     
        if name.strip() == "":
            continue
        
        result += [name]

    return result


def make_index_of_authors():
    result = """
* [# H4ckers]
"""
    authors = list_authors()

    #en_char = "abcdefghijklmnopqrstuvwxyz"
    data_dict = {
        "abcd": [],
        "efgh": [],
        "ijkl": [],
        "mnop": [],
        "qrst": [],
        "uvwx": [],
        "yz": [],
    }
    data_dict_nonen = []

    en_keys = data_dict.keys()
    en_keys.sort()  # 這裡作一次排序，以避免字母順序錯誤

    for author in authors:
        isNonEn = True
        for key in en_keys:
            if author[0].lower() in key and not author in data_dict[key]:
                    data_dict[key] += [author]
                    isNonEn = False
        if isNonEn:
            data_dict_nonen += [author]

    for key in en_keys:
        result += """ * [# 字母%s-%s]
""" % (key[:1].upper(), key[-1:].upper())
        items = data_dict[key]
        items.sort()  # 這裡作一次排序
        for item in items:

            # 這裡將作者名稱再轉成更個人化一些
            author_data = find_author_by_name( item )
            rel_name = author_data['rel_name']

            if item.lower() == rel_name.lower() or rel_name.strip() == "":
		    name = "%s" % ( item )
            else:
		    name = "%s (%s)" % ( rel_name, item )

            result += "  * [[[user:" + item + "|" + name + "]]]\n"

    # 目前 wikidot 不支援中文條目
    result += """ * [# 非英文字母]
  * [[[user:non-en]]]
"""

    for item in data_dict_nonen:
        print "非英文項目: ", item

    return result


def process_page_per_author(author):
    page_content = """
[[toc]]
"""

    for cc in collection:
        name = cc[0]
        date = cc[1]
        rows = cc[2]
        if name.lower() == author.lower():
            page_content += "\n+ " + date + "\n"
            page_content += "來源: [[[" + date + "]]]\n"
            for row in rows:
                page_content += row + "\n"

    page_content = page_content.strip() # Normalize

    return page_content

def wikidot_list_date_pages( proxy, site ):
        ret_data = []

        pages = proxy.pages.select({'site': site})
        for page in pages:
            if page[0:2] == "20" and page[4] == "-" and page[7] == "-":
                ret_data += [page]

        return ret_data
        
def wikidot_list_user_pages( proxy, site ):
        ret_data = []

        pages = proxy.pages.select({'site': site})
        for page in pages:
            if page[0:5] == "user:":
                ret_data += [page]

        return ret_data

def page_split_by_keyword( text, keyword ):
        pos = text.rfind(keyword)

        if pos >= 0:
            head = text[:pos]
            body = text[pos:]
        else:
            head = text
            body = ""

        return head, body

#
# Main routine below
#


# 列出要處理的頁面
user_pages = wikidot_list_user_pages( proxy, site )
date_pages = wikidot_list_date_pages( proxy, site )
print "Fetching 聚會手記..."
pull_and_cache_pages(site, date_pages )  # 下載聚會手記相關的頁面
print "Fetching 個人頁面..."
pull_and_cache_pages(site, user_pages )  # 下載個人頁面相關的頁面

date_pages.sort(reverse=True)  # 將最新日期放在最前面
for page in date_pages:
    print "處理頁面: ", page
    parse_data_from_page(page)

# 
# 列出每個作者的統計資訊
# 
print "==== 列出每個作者的統計資訊 ============================"
authors = list_authors()
authors_cnt = [] 
for author in authors:
    cnt = 0 
    for cc in collection:
        name = cc[0]
	if name == author:
		cnt += 1
    authors_cnt += [ ( author, cnt ) ]

authors_cnt.sort( key=lambda x: x[1], reverse=True )

for row in authors_cnt:
    print "%s  %s" % ( str(row[1]).rjust(4), row[0] ) 

#
# 更新個別作者的頁面
#
print "==== 更新個別作者的頁面 ========================="
authors = list_authors()
for author in authors:
    page_of_author = process_page_per_author(author)
    page = "user:" + author.lower()
    user_head, page_of_author_orig = page_split_by_keyword( get_page_content(page), "[[toc]]" )

    if page_of_author == page_of_author_orig:
        # 內容沒有變動，跳過不更新
        continue

    # 顯示差異的部分
    tmp_old = tempfile.mktemp()
    tmp_new = tempfile.mktemp()
    string2file( page_of_author_orig, tmp_old )
    string2file( page_of_author, tmp_new )
    cmd = "diff -Naur %s %s" % ( tmp_old, tmp_new )
    out = subprocess.Popen( cmd, shell=True, stdout=subprocess.PIPE ).communicate()[0]
    os.unlink( tmp_old )
    os.unlink( tmp_new )
    print "==== %s 新舊差異的部分 ===========" % author
    print out
    print "=================================="

    page_of_author = user_head + page_of_author # 加上保留的使用者編輯內容

    author_data = find_author_by_name( author )
    name = author_data['rel_name']

    title = name + "的聚會手記"

    print "更新個人頁面: %s/%s" % ( root_url, page )
    proxy.pages.save_one({'site': site, 'page': page, 'title': title, 'content': page_of_author})
    collect_notify( author )

#
# Calucate user statics
#
ary_local  = list_authors()
ary_remote = []
for user_page in user_pages:
	user = user_page[5:]
	ary_remote += [ user ]

ary_all = set( ary_local + ary_remote )

ary_local_gt_remote = []
ary_local_eq_remote = []
ary_local_lt_remote = []

for item in ary_all:
	if item in ary_local and item in ary_remote:
		ary_local_eq_remote += [ item ]
	elif item in ary_local and not item in ary_remote:
		ary_local_gt_remote += [ item ]
	elif not item in ary_local and item in ary_remote:
		ary_local_lt_remote += [ item ]
	else:
		print "例外: %s" % item 

print "== 本地有，遠端有 ======================"
for x in ary_local_eq_remote:
	print "%s/user:%s" % ( root_url, x )
print "== 本地有，遠端沒有 ===================="
for x in ary_local_gt_remote:
	print "%s/user:%s" % ( root_url, x )
print "== 本地沒有，遠端有 ===================="
for x in ary_local_lt_remote:
	print "%s/user:%s" % ( root_url, x )
print "========================================"


# 展開作者列表
page_of_index = make_index_of_authors()
print page_of_index

# 將作者列表塞到主選單中
nav_page = "nav:top"
keyword_beg = "\n* [# H4ckers]\n"
keyword_end = "\n  * [[[user:non-en]]]\n"

nav_top_page = proxy.pages.get_one({"site": site, "page": nav_page})
nav_top_content = nav_top_page["content"].encode('utf8')
k_start = nav_top_content.find(keyword_beg)
k_end = nav_top_content.find(keyword_end)

if k_start >= 0 and k_end >= 0 and k_end > k_start:
    k_end = k_end + keyword_end.__len__()

    new_nav_top_content = nav_top_content[0:k_start] + page_of_index + nav_top_content[k_end:]

    #print new_nav_top_content
    proxy.pages.save_one({'site': site, 'page': nav_page, 'content': new_nav_top_content})


#
# 通知作者有頁面更新
#
print "==== 通知作者有頁面更新 =================================="
for author in collection3:
    author_data = find_author_by_name( author )
    rel_name = author_data['rel_name']
    url_name = author_data['url_name']
    email    = author_data['email']

    print 'To nofity: "%s" <%s> , %s/user:%s ' % ( rel_name, email, root_url, url_name )
